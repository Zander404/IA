{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#  For Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# For Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "# For Styling\n",
    "plt.style.use('default')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\") # Nome do CSV a ser usado/ conjunto de dados\n",
    "\n",
    "## Caso a base for muito grande usamos apenas um peda√ßo do data-set\n",
    "# df = df.sample(frac=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_str(tweet):\n",
    "    sentence = ''\n",
    "    for words in tweet:\n",
    "        sentence += words\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def get_str(lst):\n",
    "    sentence = ''\n",
    "    for char in lst:\n",
    "        sentence += char+' '\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def get_word(text): \n",
    "    result = nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "    return result\n",
    "\n",
    "def stopword_list(stop):\n",
    "    lst = stopwords.words('english')\n",
    "    for stopword in stop:\n",
    "        lst.append(stopword)\n",
    "    return lst\n",
    "\n",
    "def remove_stopword(stopwords, lst):    \n",
    "    stoplist = stopword_list(stopwords)\n",
    "    txt = ''\n",
    "    for idx in range(len(lst)):\n",
    "        txt += lst[idx]\n",
    "        txt += '\\n'\n",
    "    cleanwordlist = [word for word in txt.split() if word not in stoplist] \n",
    "#     print(stoplist)\n",
    "    return cleanwordlist\n",
    "\n",
    "def pos_remove_noun(tagged):\n",
    "    remove_noun = [word for word,pos in tagged if pos not in ['NN','NNS','NNP','NNPS']]\n",
    "    txt = ''\n",
    "    for i in range(len(remove_noun)):\n",
    "        txt += remove_noun[i]\n",
    "        txt += '\\n'\n",
    "    return cleanwordlist\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(word) for word in words]\n",
    "#     txt= ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def Freq_df(cleanwordlist):\n",
    "    Freq_dist_nltk = nltk.FreqDist(cleanwordlist)\n",
    "    df_freq = pd.DataFrame.from_dict(Freq_dist_nltk, orient='index')\n",
    "    df_freq.columns = ['Frequency']\n",
    "    df_freq.index.name = 'Term'\n",
    "    df_freq = df_freq.sort_values(by=['Frequency'],ascending=False)\n",
    "    df_freq = df_freq.reset_index()\n",
    "    return df_freq\n",
    "\n",
    "def Word_Cloud(data, color_background, colormap, title):\n",
    "    plt.figure(figsize = (15,20))\n",
    "    wc = WordCloud(width=800, \n",
    "               height=400, \n",
    "               max_words=50,\n",
    "               colormap= colormap,\n",
    "               max_font_size=140,\n",
    "               random_state=8888, \n",
    "               background_color=color_background).generate_from_frequencies(data)\n",
    "    \n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def whole_string(sent):\n",
    "    all_str = get_all_str(sent)\n",
    "    words = get_word(all_str)\n",
    "    removed = remove_stopword(['http','https','co'],words)\n",
    "    freq_df = Freq_df(removed)\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    words = get_word(text)\n",
    "    lemma = lemmatization(words)\n",
    "    removed = remove_stopword(['http','ly'],lemma)\n",
    "    return removed\n",
    "\n",
    "df['OriginalTweet'] = df['OriginalTweet'].apply(preprocessing)\n",
    "df['OriginalTweet'] = df['OriginalTweet'].apply(get_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vector = CountVectorizer()\n",
    "X = vector.fit(df['OriginalTweet'])\n",
    "X_transform = X.transform(df['OriginalTweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf = TfidfTransformer()\n",
    "tfidf_transformer = Tfidf.fit(X_transform)\n",
    "X = tfidf_transformer.transform(X_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log = LogisticRegression(random_state=42)\n",
    "log.fit(X_train, y_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
