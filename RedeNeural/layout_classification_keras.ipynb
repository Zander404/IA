{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.datasets import boston_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8991 entries, 29256 to 21571\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       8991 non-null   int64 \n",
      " 1   ScreenName     8991 non-null   int64 \n",
      " 2   Location       7135 non-null   object\n",
      " 3   TweetAt        8991 non-null   object\n",
      " 4   OriginalTweet  8991 non-null   object\n",
      " 5   Sentiment      8991 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 491.7+ KB\n"
     ]
    }
   ],
   "source": [
    "teste = pd.read_csv(\"datasets/Classification/Corona_NLP_test.csv\")\n",
    "treino = pd.read_csv(\"datasets/Classification/Corona_NLP_train.csv\",encoding='latin-1')\n",
    "\n",
    "data = pd.concat([teste,treino])\n",
    "data = data.sample(frac=0.2, random_state=42)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=[ \"UserName\",\"ScreenName\",\"Location\",\"TweetAt\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_sentiment(score):\n",
    "    if score == 'Negative':\n",
    "        return \"Negative\"\n",
    "    elif score == 'Extremely Negative':\n",
    "        return \"Negative\"\n",
    "    elif score == 'Positive':\n",
    "        return \"Positive\"\n",
    "    elif score == 'Extremely Positive':\n",
    "        return \"Positive\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "    \n",
    "    \n",
    "data['Sentiment'] = data['Sentiment'].apply(categorize_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fazer TOKENIZACAO das palavras\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_all_str(tweet):\n",
    "    sentence = ''\n",
    "    for words in tweet:\n",
    "        sentence += words\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "def get_str(lst):\n",
    "    sentence = ''\n",
    "    for char in lst:\n",
    "        sentence += char+' '\n",
    "    sentence = sentence.lower()\n",
    "    return sentence\n",
    "\n",
    "def get_word(text): \n",
    "    result = nltk.RegexpTokenizer(r'\\w+').tokenize(text.lower())\n",
    "    return result\n",
    "\n",
    "def get_hashtag(text):\n",
    "    result = nltk.RegexpTokenizer(r'(?<=#)\\w+').tokenize(text.lower())\n",
    "    return result\n",
    "\n",
    "def get_mention(text):\n",
    "    result = nltk.RegexpTokenizer(r'(?<=@)\\w+').tokenize(text.lower())\n",
    "    return result \n",
    "\n",
    "def stopword_list(stop):\n",
    "    lst = stopwords.words('english')\n",
    "    for stopword in stop:\n",
    "        lst.append(stopword)\n",
    "    return lst\n",
    "\n",
    "def remove_stopword(stopwords, lst):    \n",
    "    stoplist = stopword_list(stopwords)\n",
    "    txt = ''\n",
    "    for idx in range(len(lst)):\n",
    "        txt += lst[idx]\n",
    "        txt += '\\n'\n",
    "    cleanwordlist = [word for word in txt.split() if word not in stoplist] \n",
    "#     print(stoplist)\n",
    "    return cleanwordlist\n",
    "\n",
    "def pos_remove_noun(tagged):\n",
    "    remove_noun = [word for word,pos in tagged if pos not in ['NN','NNS','NNP','NNPS']]\n",
    "    txt = ''\n",
    "    for i in range(len(remove_noun)):\n",
    "        txt += remove_noun[i]\n",
    "        txt += '\\n'\n",
    "    return cleanwordlist\n",
    "\n",
    "def lemmatization(words):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    tokens = [lemm.lemmatize(word) for word in words]\n",
    "#     txt= ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    words = get_word(text)\n",
    "    lemma = lemmatization(words)\n",
    "    removed = remove_stopword(['http','ly'],lemma)\n",
    "    return removed\n",
    "\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(preprocessing)\n",
    "data['OriginalTweet'] = data['OriginalTweet'].apply(get_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=[\"Sentiment\"])\n",
    "y = data[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "Tfidf = TfidfVectorizer(stop_words='english', norm='l2' )\n",
    "X = Tfidf.fit_transform(X[\"OriginalTweet\"]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import to_categorical\n",
    "# y = to_categorical(y, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 2, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the model\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization\n",
    "# from keras.initializers import glorot_uniform\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(128, input_dim=X_train.shape[1], activation='relu', kernel_initializer=glorot_uniform() ))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(3, activation='sigmoid'))  # Softmax para classificação de 3 classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, LayerNormalization\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X.shape[1], activation='relu', kernel_initializer=glorot_uniform()))\n",
    "    model.add(Dropout(0.2, seed=42))\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(len(np.unique(y)), activation='softmax'))\n",
    "    model.compile(optimizer=Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 9ms/step - loss: 1.5400 - accuracy: 0.6620\n",
      "Loss: 1.5400452613830566, Accuracy: 0.6620344519615173\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.4039 - accuracy: 0.7047\n",
      "Loss: 1.4039355516433716, Accuracy: 0.7046718597412109\n",
      "57/57 [==============================] - 1s 12ms/step - loss: 1.4985 - accuracy: 0.6846\n",
      "Loss: 1.498525619506836, Accuracy: 0.6846495866775513\n",
      "57/57 [==============================] - 1s 12ms/step - loss: 1.5911 - accuracy: 0.6752\n",
      "Loss: 1.5911248922348022, Accuracy: 0.6751946806907654\n",
      "57/57 [==============================] - 1s 11ms/step - loss: 1.4971 - accuracy: 0.6674\n",
      "Loss: 1.497111201286316, Accuracy: 0.6674082279205322\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0)\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6674082279205322\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 448ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 1.  ],\n",
       "       [0.21, 0.76, 0.03],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.12, 0.  , 0.87],\n",
       "       [0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.67, 0.33],\n",
       "       [0.19, 0.  , 0.81],\n",
       "       [0.  , 0.  , 1.  ],\n",
       "       [1.  , 0.  , 0.  ]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[0:10]\n",
    "y_preds = model.predict(X_new)\n",
    "y_preds.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 2, 2, 0, 1, 0, 2, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
